{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laylaelbaaly/NeuralStyleTransfer/blob/main/IOB_NST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0dkKLIHaoG"
      },
      "source": [
        "**IOB-NST Algorithm based on the origial paper “A Neural Algorithm of Artistic Style” by Gatys et al.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRxBUpOMuPy",
        "outputId": "955e3046-bf8e-433c-f8f1-ae426de4814c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIRgYM61TGju",
        "outputId": "e4d955d5-3ecd-4e3d-cd5c-2337b88b2448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbEG4D1EG0oS"
      },
      "outputs": [],
      "source": [
        "# ===========================\n",
        "# IMPORT REQUIRED LIBRARIES\n",
        "# ===========================\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "# from keras.applications import vgg19\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import vgg19\n",
        "from tensorflow.keras.models import Model\n",
        "K = tf.keras.backend\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from PIL import Image\n",
        "\n",
        "# ==============================\n",
        "# GLOBAL SETTINGS AND CONSTANTS\n",
        "# ==============================\n",
        "\n",
        "img_height = 512\n",
        "result_prefix = \"results\"\n",
        "\n",
        "# Weighting factors for each loss term\n",
        "# content_weight = 0.025\n",
        "# style_weight = 1.0\n",
        "# total_variation_weight = 1e-4\n",
        "\n",
        "# Define dataset directories\n",
        "\n",
        "# Set content and style image folders in Google Drive\n",
        "content_dir = '/content/drive/MyDrive/University of London/Year 3/FINAL PROJECT/Final Project Code/IOB-NST/content'\n",
        "style_dir = '/content/drive/MyDrive/University of London/Year 3/FINAL PROJECT/Final Project Code/IOB-NST/style'\n",
        "\n",
        "# Output folder (where stylized images will be saved)\n",
        "output_dir = '/content/drive/MyDrive/University of London/Year 3/FINAL PROJECT/Final Project Code/IOB-NST/stylized_outputs'\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNG8x3hyLueK"
      },
      "outputs": [],
      "source": [
        "# ===================================\n",
        "# IMAGE PROCESSING UTILITY FUNCTIONS\n",
        "# ===================================\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image for VGG19.\n",
        "    \"\"\"\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return vgg19.preprocess_input(img)\n",
        "\n",
        "def deprocess_image(x):\n",
        "    \"\"\"\n",
        "    Reverses VGG19 preprocessing to return a displayable image.\n",
        "    \"\"\"\n",
        "    x = x.reshape((img_height, img_width, 3))\n",
        "    x[:, :, 0] += 103.939  # Add mean RGB values (reverse normalization)\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    x = x[:, :, ::-1]      # Convert BGR to RGB\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return Image.fromarray(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXUphJH1Lu0p"
      },
      "outputs": [],
      "source": [
        "# ==============================\n",
        "# LOSS FUNCTION DEFINITIONS\n",
        "# ==============================\n",
        "\n",
        "def content_loss(base, combination):\n",
        "    \"\"\"\n",
        "    Measures content similarity (high-level feature maps).\n",
        "    \"\"\"\n",
        "    return K.sum(K.square(combination - base))\n",
        "\n",
        "def gram_matrix(x):\n",
        "    \"\"\"\n",
        "    Computes the Gram matrix (feature correlation matrix).\n",
        "    \"\"\"\n",
        "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "    return K.dot(features, K.transpose(features))\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    \"\"\"\n",
        "    Measures style similarity via Gram matrix difference.\n",
        "    \"\"\"\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_height * img_width\n",
        "    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "def total_variation_loss(x):\n",
        "    \"\"\"\n",
        "    Encourages spatial smoothness in the generated image.\n",
        "    \"\"\"\n",
        "    a = K.square(x[:, :img_height-1, :img_width-1, :] - x[:, 1:, :img_width-1, :])\n",
        "    b = K.square(x[:, :img_height-1, :img_width-1, :] - x[:, :img_height-1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsr-qh_9Lu3R"
      },
      "outputs": [],
      "source": [
        "# ===================================\n",
        "# EVALUATOR: WRAPS LOSS AND GRADIENT\n",
        "# ===================================\n",
        "\n",
        "class Evaluator(object):\n",
        "    \"\"\"\n",
        "    Used to interface with L-BFGS optimizer: returns loss and gradient separately,\n",
        "    while caching them to avoid recomputation.\n",
        "    \"\"\"\n",
        "    def __init__(self, fetch_loss_and_grads_fn):\n",
        "        self.fetch_loss_and_grads = fetch_loss_and_grads_fn\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        \"\"\"\n",
        "        Returns the loss value and caches the gradients.\n",
        "        \"\"\"\n",
        "        x = x.reshape((1, img_height, img_width, 3))\n",
        "        outs = self.fetch_loss_and_grads([x])\n",
        "        self.loss_value = outs[0]\n",
        "        self.grads_values = outs[1].flatten().astype('float64')\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        \"\"\"\n",
        "        Returns cached gradients from the last loss computation.\n",
        "        \"\"\"\n",
        "        grads = np.copy(self.grads_values)\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK4q3iglLu6a"
      },
      "outputs": [],
      "source": [
        "# ===================================\n",
        "# NEURAL STYLE TRANSFER FUNCTION\n",
        "# ===================================\n",
        "\n",
        "def run_style_transfer(content_path, style_path, output_path,\n",
        "                       content_weight, style_weight, total_variation_weight,\n",
        "                       iterations=300):\n",
        "\n",
        "    global img_width\n",
        "    width, height = load_img(content_path).size\n",
        "    img_width = int(width * img_height / height)\n",
        "\n",
        "    # Preprocess content and style images\n",
        "    content_image = preprocess_image(content_path, (img_height, img_width))\n",
        "    style_image = preprocess_image(style_path, (img_height, img_width))\n",
        "\n",
        "    # Initialize the combination image as a trainable variable\n",
        "    combination_image = tf.Variable(content_image, dtype=tf.float32)\n",
        "\n",
        "    # Load VGG19 and freeze layers\n",
        "    vgg = vgg19.VGG19(weights='imagenet', include_top=False)\n",
        "    vgg.trainable = False\n",
        "\n",
        "    # Layers to extract features from\n",
        "    content_layer = 'block5_conv2'\n",
        "    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "    output_layers = [content_layer] + style_layers\n",
        "\n",
        "    # Build model that outputs selected layer activations\n",
        "    outputs = [vgg.get_layer(name).output for name in output_layers]\n",
        "    feature_extractor = Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "    # Extract features for content and style images\n",
        "    content_features = feature_extractor(content_image)[0]\n",
        "    style_features = feature_extractor(style_image)[1:]\n",
        "\n",
        "    # Updated Gram matrix computation for 4D tensors\n",
        "    def gram_matrix(tensor):\n",
        "        \"\"\"\n",
        "        Computes the Gram matrix from a 4D feature map.\n",
        "        Input shape: (1, height, width, channels)\n",
        "        Output shape: (1, channels, channels)\n",
        "        \"\"\"\n",
        "        assert len(tensor.shape) == 4, \"Input must be 4D (batch, h, w, c)\"\n",
        "        batch_size, height, width, channels = tensor.shape\n",
        "        x = tf.reshape(tensor, [batch_size, height * width, channels])\n",
        "        gram = tf.matmul(x, x, transpose_a=True)\n",
        "        return gram / tf.cast(height * width, tf.float32)\n",
        "\n",
        "    # Loss function + gradient computation\n",
        "    def compute_loss_and_grads():\n",
        "        with tf.GradientTape() as tape:\n",
        "            combo_outputs = feature_extractor(combination_image)\n",
        "            combo_content = combo_outputs[0]\n",
        "            combo_styles = combo_outputs[1:]\n",
        "\n",
        "            # Content loss: how much the content changes\n",
        "            c_loss = content_weight * tf.reduce_mean(tf.square(combo_content - content_features))\n",
        "\n",
        "            # Style loss: how much the textures match\n",
        "\n",
        "            # s_loss = 0\n",
        "            # for style_feat, combo_feat in zip(style_features, combo_styles):\n",
        "            #     S = gram_matrix(style_feat)\n",
        "            #     C = gram_matrix(combo_feat)\n",
        "            #     channels = 3\n",
        "            #     size = img_height * img_width\n",
        "            #     s_loss += tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "            # s_loss *= style_weight / len(style_layers)\n",
        "\n",
        "            s_loss = 0\n",
        "            for style_feat, combo_feat in zip(style_features, combo_styles):\n",
        "                S = gram_matrix(style_feat)\n",
        "                C = gram_matrix(combo_feat)\n",
        "                shape = tf.shape(combo_feat)\n",
        "                channels = tf.cast(shape[-1], tf.float32)\n",
        "                size = tf.cast(shape[1] * shape[2], tf.float32)\n",
        "                s_loss += tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
        "            s_loss *= style_weight / len(style_layers)\n",
        "\n",
        "\n",
        "            # Total variation loss: encourages spatial smoothness\n",
        "            tv_loss = total_variation_weight * total_variation_loss(combination_image)\n",
        "\n",
        "            # Final combined loss\n",
        "            total_loss = c_loss + s_loss + tv_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, combination_image)\n",
        "        return total_loss, grads\n",
        "\n",
        "    # Use Adam optimizer\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=10.0)\n",
        "\n",
        "    # Optimization loop\n",
        "    for i in range(iterations):\n",
        "        loss, grads = compute_loss_and_grads()\n",
        "        optimizer.apply_gradients([(grads, combination_image)])\n",
        "        print(f\"Iteration {i+1}, Loss: {loss.numpy():,.2f}\")\n",
        "\n",
        "    # Save the stylized image\n",
        "    final_img = deprocess_image(combination_image.numpy())\n",
        "    final_img.save(output_path)\n",
        "    print(f\"Saved: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QasjUReQLvTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4176a50-dfb8-40b9-f686-103b15d7b4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: modern-residential-building-beautiful-recreation-area-modern-residential-building-recreation-area-131403554.jpg + 83316.jpg\n",
            "Iteration 1, Loss: 7,610,693.00\n",
            "Iteration 2, Loss: 3,187,717.00\n",
            "Iteration 3, Loss: 3,205,992.75\n",
            "Iteration 4, Loss: 2,346,490.00\n",
            "Iteration 5, Loss: 1,622,154.00\n",
            "Iteration 6, Loss: 1,696,455.12\n",
            "Iteration 7, Loss: 1,225,491.12\n",
            "Iteration 8, Loss: 1,247,440.12\n",
            "Iteration 9, Loss: 1,020,615.25\n",
            "Iteration 10, Loss: 1,016,860.56\n",
            "Iteration 11, Loss: 845,503.62\n",
            "Iteration 12, Loss: 792,300.81\n",
            "Iteration 13, Loss: 739,252.31\n",
            "Iteration 14, Loss: 655,652.88\n",
            "Iteration 15, Loss: 653,057.88\n",
            "Iteration 16, Loss: 562,113.19\n",
            "Iteration 17, Loss: 560,567.12\n",
            "Iteration 18, Loss: 498,650.84\n",
            "Iteration 19, Loss: 485,397.69\n",
            "Iteration 20, Loss: 456,224.28\n",
            "Iteration 21, Loss: 425,749.88\n",
            "Iteration 22, Loss: 414,362.66\n",
            "Iteration 23, Loss: 382,797.94\n",
            "Iteration 24, Loss: 378,824.31\n",
            "Iteration 25, Loss: 349,196.03\n",
            "Iteration 26, Loss: 345,321.75\n",
            "Iteration 27, Loss: 321,883.84\n",
            "Iteration 28, Loss: 317,286.12\n",
            "Iteration 29, Loss: 300,466.59\n",
            "Iteration 30, Loss: 292,400.56\n",
            "Iteration 31, Loss: 281,306.28\n",
            "Iteration 32, Loss: 272,764.75\n",
            "Iteration 33, Loss: 264,500.88\n",
            "Iteration 34, Loss: 256,692.52\n",
            "Iteration 35, Loss: 249,378.28\n",
            "Iteration 36, Loss: 242,517.97\n",
            "Iteration 37, Loss: 236,699.05\n",
            "Iteration 38, Loss: 230,093.08\n",
            "Iteration 39, Loss: 225,067.73\n",
            "Iteration 40, Loss: 219,709.69\n",
            "Iteration 41, Loss: 214,656.75\n",
            "Iteration 42, Loss: 210,313.30\n",
            "Iteration 43, Loss: 205,275.61\n",
            "Iteration 44, Loss: 202,212.16\n",
            "Iteration 45, Loss: 197,059.75\n",
            "Iteration 46, Loss: 194,454.41\n",
            "Iteration 47, Loss: 189,864.27\n",
            "Iteration 48, Loss: 187,507.36\n",
            "Iteration 49, Loss: 183,610.19\n",
            "Iteration 50, Loss: 180,930.41\n",
            "Iteration 51, Loss: 178,023.27\n",
            "Iteration 52, Loss: 175,038.25\n",
            "Iteration 53, Loss: 172,870.23\n",
            "Iteration 54, Loss: 169,894.75\n",
            "Iteration 55, Loss: 167,920.94\n",
            "Iteration 56, Loss: 165,393.11\n",
            "Iteration 57, Loss: 163,080.92\n",
            "Iteration 58, Loss: 161,305.62\n",
            "Iteration 59, Loss: 158,886.23\n",
            "Iteration 60, Loss: 157,184.83\n",
            "Iteration 61, Loss: 155,276.44\n",
            "Iteration 62, Loss: 153,290.41\n",
            "Iteration 63, Loss: 151,768.56\n",
            "Iteration 64, Loss: 149,927.52\n",
            "Iteration 65, Loss: 148,304.53\n",
            "Iteration 66, Loss: 146,843.77\n",
            "Iteration 67, Loss: 145,228.48\n",
            "Iteration 68, Loss: 143,759.52\n",
            "Iteration 69, Loss: 142,437.97\n",
            "Iteration 70, Loss: 141,001.52\n",
            "Iteration 71, Loss: 139,661.77\n",
            "Iteration 72, Loss: 138,478.56\n",
            "Iteration 73, Loss: 137,196.41\n",
            "Iteration 74, Loss: 135,968.78\n",
            "Iteration 75, Loss: 134,865.28\n",
            "Iteration 76, Loss: 133,752.28\n",
            "Iteration 77, Loss: 132,589.69\n",
            "Iteration 78, Loss: 131,530.08\n",
            "Iteration 79, Loss: 130,513.70\n",
            "Iteration 80, Loss: 129,485.95\n",
            "Iteration 81, Loss: 128,471.05\n",
            "Iteration 82, Loss: 127,522.88\n",
            "Iteration 83, Loss: 126,613.47\n",
            "Iteration 84, Loss: 125,697.53\n",
            "Iteration 85, Loss: 124,774.66\n",
            "Iteration 86, Loss: 123,893.47\n",
            "Iteration 87, Loss: 123,040.48\n",
            "Iteration 88, Loss: 122,191.13\n",
            "Iteration 89, Loss: 121,346.59\n",
            "Iteration 90, Loss: 120,505.31\n",
            "Iteration 91, Loss: 119,689.51\n",
            "Iteration 92, Loss: 118,903.44\n",
            "Iteration 93, Loss: 118,147.06\n",
            "Iteration 94, Loss: 117,402.83\n",
            "Iteration 95, Loss: 116,663.20\n",
            "Iteration 96, Loss: 115,939.68\n",
            "Iteration 97, Loss: 115,237.52\n",
            "Iteration 98, Loss: 114,554.22\n",
            "Iteration 99, Loss: 113,884.84\n",
            "Iteration 100, Loss: 113,231.95\n",
            "Iteration 101, Loss: 112,592.37\n",
            "Iteration 102, Loss: 111,965.43\n",
            "Iteration 103, Loss: 111,351.88\n",
            "Iteration 104, Loss: 110,748.09\n",
            "Iteration 105, Loss: 110,162.69\n",
            "Iteration 106, Loss: 109,601.71\n",
            "Iteration 107, Loss: 109,085.99\n",
            "Iteration 108, Loss: 108,660.30\n",
            "Iteration 109, Loss: 108,454.52\n",
            "Iteration 110, Loss: 108,793.52\n",
            "Iteration 111, Loss: 110,433.95\n",
            "Iteration 112, Loss: 115,481.88\n",
            "Iteration 113, Loss: 125,780.47\n",
            "Iteration 114, Loss: 147,169.31\n",
            "Iteration 115, Loss: 155,527.64\n",
            "Iteration 116, Loss: 146,973.78\n",
            "Iteration 117, Loss: 109,536.91\n",
            "Iteration 118, Loss: 113,986.62\n",
            "Iteration 119, Loss: 133,950.84\n",
            "Iteration 120, Loss: 111,751.62\n",
            "Iteration 121, Loss: 107,573.09\n",
            "Iteration 122, Loss: 121,669.43\n",
            "Iteration 123, Loss: 107,324.20\n",
            "Iteration 124, Loss: 106,397.99\n",
            "Iteration 125, Loss: 114,766.59\n",
            "Iteration 126, Loss: 103,412.54\n",
            "Iteration 127, Loss: 106,126.79\n",
            "Iteration 128, Loss: 110,002.95\n",
            "Iteration 129, Loss: 100,984.31\n",
            "Iteration 130, Loss: 105,473.69\n",
            "Iteration 131, Loss: 106,160.83\n",
            "Iteration 132, Loss: 99,500.97\n",
            "Iteration 133, Loss: 104,157.32\n",
            "Iteration 134, Loss: 102,986.73\n",
            "Iteration 135, Loss: 98,276.09\n",
            "Iteration 136, Loss: 102,286.96\n",
            "Iteration 137, Loss: 100,546.71\n",
            "Iteration 138, Loss: 97,085.74\n",
            "Iteration 139, Loss: 100,177.28\n",
            "Iteration 140, Loss: 98,740.99\n",
            "Iteration 141, Loss: 95,866.13\n",
            "Iteration 142, Loss: 98,082.23\n",
            "Iteration 143, Loss: 97,363.08\n",
            "Iteration 144, Loss: 94,726.72\n",
            "Iteration 145, Loss: 96,017.28\n",
            "Iteration 146, Loss: 96,221.36\n",
            "Iteration 147, Loss: 93,860.86\n",
            "Iteration 148, Loss: 93,943.98\n",
            "Iteration 149, Loss: 94,899.74\n",
            "Iteration 150, Loss: 93,423.66\n",
            "Iteration 151, Loss: 92,260.67\n",
            "Iteration 152, Loss: 92,915.53\n",
            "Iteration 153, Loss: 92,953.20\n",
            "Iteration 154, Loss: 91,700.22\n",
            "Iteration 155, Loss: 91,036.22\n",
            "Iteration 156, Loss: 91,463.09\n",
            "Iteration 157, Loss: 91,710.02\n",
            "Iteration 158, Loss: 91,209.29\n",
            "Iteration 159, Loss: 91,161.12\n",
            "Iteration 160, Loss: 92,600.77\n",
            "Iteration 161, Loss: 96,538.16\n",
            "Iteration 162, Loss: 101,145.91\n",
            "Iteration 163, Loss: 107,825.88\n",
            "Iteration 164, Loss: 103,718.31\n",
            "Iteration 165, Loss: 98,241.82\n",
            "Iteration 166, Loss: 94,090.48\n",
            "Iteration 167, Loss: 98,296.14\n",
            "Iteration 168, Loss: 99,151.73\n",
            "Iteration 169, Loss: 90,127.90\n",
            "Iteration 170, Loss: 88,755.07\n",
            "Iteration 171, Loss: 94,332.51\n",
            "Iteration 172, Loss: 91,655.34\n",
            "Iteration 173, Loss: 88,292.42\n",
            "Iteration 174, Loss: 90,962.98\n",
            "Iteration 175, Loss: 90,228.98\n",
            "Iteration 176, Loss: 86,767.07\n",
            "Iteration 177, Loss: 87,058.02\n",
            "Iteration 178, Loss: 88,198.23\n",
            "Iteration 179, Loss: 86,448.04\n",
            "Iteration 180, Loss: 85,470.91\n",
            "Iteration 181, Loss: 86,781.05\n",
            "Iteration 182, Loss: 86,808.14\n",
            "Iteration 183, Loss: 85,738.81\n",
            "Iteration 184, Loss: 86,699.66\n",
            "Iteration 185, Loss: 89,437.52\n",
            "Iteration 186, Loss: 91,629.11\n",
            "Iteration 187, Loss: 97,612.86\n",
            "Iteration 188, Loss: 105,498.82\n",
            "Iteration 189, Loss: 117,859.23\n",
            "Iteration 190, Loss: 111,656.60\n",
            "Iteration 191, Loss: 96,442.05\n",
            "Iteration 192, Loss: 85,092.07\n",
            "Iteration 193, Loss: 92,976.38\n",
            "Iteration 194, Loss: 97,919.42\n",
            "Iteration 195, Loss: 89,536.27\n",
            "Iteration 196, Loss: 88,146.42\n",
            "Iteration 197, Loss: 90,248.67\n",
            "Iteration 198, Loss: 87,065.44\n",
            "Iteration 199, Loss: 86,157.42\n",
            "Iteration 200, Loss: 87,706.71\n",
            "Iteration 201, Loss: 85,102.81\n",
            "Iteration 202, Loss: 83,672.23\n",
            "Iteration 203, Loss: 86,159.63\n",
            "Iteration 204, Loss: 84,540.52\n",
            "Iteration 205, Loss: 81,836.74\n",
            "Iteration 206, Loss: 83,566.52\n",
            "Iteration 207, Loss: 83,704.91\n",
            "Iteration 208, Loss: 81,641.37\n",
            "Iteration 209, Loss: 81,976.05\n",
            "Iteration 210, Loss: 81,906.85\n",
            "Iteration 211, Loss: 80,822.55\n",
            "Iteration 212, Loss: 81,076.94\n",
            "Iteration 213, Loss: 80,874.99\n",
            "Iteration 214, Loss: 80,225.98\n",
            "Iteration 215, Loss: 80,391.59\n",
            "Iteration 216, Loss: 80,034.85\n",
            "Iteration 217, Loss: 79,298.52\n",
            "Iteration 218, Loss: 79,515.66\n",
            "Iteration 219, Loss: 79,607.23\n",
            "Iteration 220, Loss: 79,364.38\n",
            "Iteration 221, Loss: 80,610.40\n",
            "Iteration 222, Loss: 84,929.99\n",
            "Iteration 223, Loss: 92,639.57\n",
            "Iteration 224, Loss: 113,139.52\n",
            "Iteration 225, Loss: 124,341.92\n",
            "Iteration 226, Loss: 136,534.06\n",
            "Iteration 227, Loss: 92,590.10\n",
            "Iteration 228, Loss: 91,203.95\n",
            "Iteration 229, Loss: 124,116.45\n",
            "Iteration 230, Loss: 106,566.42\n",
            "Iteration 231, Loss: 130,940.91\n",
            "Iteration 232, Loss: 207,905.78\n",
            "Iteration 233, Loss: 157,584.78\n",
            "Iteration 234, Loss: 120,892.45\n",
            "Iteration 235, Loss: 111,604.28\n",
            "Iteration 236, Loss: 105,721.07\n",
            "Iteration 237, Loss: 125,531.52\n",
            "Iteration 238, Loss: 115,513.57\n",
            "Iteration 239, Loss: 90,906.59\n",
            "Iteration 240, Loss: 121,571.58\n",
            "Iteration 241, Loss: 103,403.91\n",
            "Iteration 242, Loss: 93,106.80\n",
            "Iteration 243, Loss: 110,791.74\n",
            "Iteration 244, Loss: 88,725.52\n",
            "Iteration 245, Loss: 98,519.20\n",
            "Iteration 246, Loss: 91,213.98\n",
            "Iteration 247, Loss: 92,216.55\n",
            "Iteration 248, Loss: 93,866.98\n",
            "Iteration 249, Loss: 86,228.55\n",
            "Iteration 250, Loss: 91,507.81\n",
            "Iteration 251, Loss: 83,701.19\n",
            "Iteration 252, Loss: 89,367.00\n",
            "Iteration 253, Loss: 85,019.77\n",
            "Iteration 254, Loss: 86,117.91\n",
            "Iteration 255, Loss: 83,999.99\n",
            "Iteration 256, Loss: 81,912.85\n",
            "Iteration 257, Loss: 83,149.91\n",
            "Iteration 258, Loss: 80,703.54\n",
            "Iteration 259, Loss: 82,526.10\n",
            "Iteration 260, Loss: 80,036.62\n",
            "Iteration 261, Loss: 80,495.64\n",
            "Iteration 262, Loss: 78,820.56\n",
            "Iteration 263, Loss: 78,720.67\n",
            "Iteration 264, Loss: 78,482.41\n",
            "Iteration 265, Loss: 78,658.42\n",
            "Iteration 266, Loss: 78,701.39\n",
            "Iteration 267, Loss: 79,246.45\n",
            "Iteration 268, Loss: 79,499.26\n",
            "Iteration 269, Loss: 80,867.88\n",
            "Iteration 270, Loss: 83,113.17\n",
            "Iteration 271, Loss: 85,038.70\n",
            "Iteration 272, Loss: 90,438.89\n",
            "Iteration 273, Loss: 89,363.86\n",
            "Iteration 274, Loss: 89,710.15\n",
            "Iteration 275, Loss: 80,824.01\n",
            "Iteration 276, Loss: 77,305.16\n",
            "Iteration 277, Loss: 79,375.47\n",
            "Iteration 278, Loss: 81,152.08\n",
            "Iteration 279, Loss: 80,126.98\n",
            "Iteration 280, Loss: 74,653.55\n",
            "Iteration 281, Loss: 75,388.89\n",
            "Iteration 282, Loss: 78,105.79\n",
            "Iteration 283, Loss: 77,762.08\n",
            "Iteration 284, Loss: 74,760.67\n",
            "Iteration 285, Loss: 72,929.66\n",
            "Iteration 286, Loss: 75,118.41\n",
            "Iteration 287, Loss: 75,665.06\n",
            "Iteration 288, Loss: 74,063.11\n",
            "Iteration 289, Loss: 72,555.99\n",
            "Iteration 290, Loss: 73,045.76\n",
            "Iteration 291, Loss: 74,047.78\n",
            "Iteration 292, Loss: 73,362.30\n",
            "Iteration 293, Loss: 72,124.96\n",
            "Iteration 294, Loss: 71,470.61\n",
            "Iteration 295, Loss: 71,959.94\n",
            "Iteration 296, Loss: 72,478.91\n",
            "Iteration 297, Loss: 72,022.93\n",
            "Iteration 298, Loss: 71,387.65\n",
            "Iteration 299, Loss: 70,882.32\n",
            "Iteration 300, Loss: 70,617.04\n",
            "Saved: /content/drive/MyDrive/University of London/Year 3/FINAL PROJECT/Final Project Code/IOB-NST/stylized_outputs/stylized_modern-residential-building-beautiful-recreation-area-modern-residential-building-recreation-area-131403554.jpg_with_83316.jpg\n",
            "\n",
            "Processing: modern-residential-building-beautiful-recreation-area-modern-residential-building-recreation-area-131403554.jpg + Van-Gogh-Starry-Night-Mural-Wallpaper_a08bca97-2248-4dbe-8ea5-5d413f142875.jpg\n",
            "Iteration 1, Loss: 18,214,760.00\n",
            "Iteration 2, Loss: 6,450,386.00\n",
            "Iteration 3, Loss: 4,932,931.50\n",
            "Iteration 4, Loss: 3,943,546.50\n",
            "Iteration 5, Loss: 2,780,926.00\n",
            "Iteration 6, Loss: 2,051,764.75\n",
            "Iteration 7, Loss: 1,667,587.38\n",
            "Iteration 8, Loss: 1,478,356.75\n",
            "Iteration 9, Loss: 1,362,635.88\n",
            "Iteration 10, Loss: 1,266,075.88\n",
            "Iteration 11, Loss: 1,166,590.00\n",
            "Iteration 12, Loss: 1,081,021.62\n",
            "Iteration 13, Loss: 977,652.19\n",
            "Iteration 14, Loss: 895,274.38\n",
            "Iteration 15, Loss: 847,160.75\n",
            "Iteration 16, Loss: 785,664.31\n",
            "Iteration 17, Loss: 737,279.00\n",
            "Iteration 18, Loss: 700,888.38\n",
            "Iteration 19, Loss: 665,783.75\n",
            "Iteration 20, Loss: 630,858.25\n",
            "Iteration 21, Loss: 597,164.50\n",
            "Iteration 22, Loss: 570,499.06\n",
            "Iteration 23, Loss: 542,840.88\n",
            "Iteration 24, Loss: 522,320.19\n",
            "Iteration 25, Loss: 501,871.84\n",
            "Iteration 26, Loss: 483,340.56\n",
            "Iteration 27, Loss: 467,837.50\n",
            "Iteration 28, Loss: 452,221.75\n",
            "Iteration 29, Loss: 437,158.31\n",
            "Iteration 30, Loss: 422,111.25\n",
            "Iteration 31, Loss: 410,555.53\n",
            "Iteration 32, Loss: 398,953.41\n",
            "Iteration 33, Loss: 387,981.72\n",
            "Iteration 34, Loss: 378,336.94\n",
            "Iteration 35, Loss: 369,536.06\n",
            "Iteration 36, Loss: 359,982.34\n",
            "Iteration 37, Loss: 351,762.50\n",
            "Iteration 38, Loss: 343,898.28\n",
            "Iteration 39, Loss: 336,915.84\n",
            "Iteration 40, Loss: 329,696.97\n",
            "Iteration 41, Loss: 323,354.97\n",
            "Iteration 42, Loss: 317,391.91\n",
            "Iteration 43, Loss: 311,481.19\n",
            "Iteration 44, Loss: 305,866.62\n",
            "Iteration 45, Loss: 300,862.22\n",
            "Iteration 46, Loss: 295,831.84\n",
            "Iteration 47, Loss: 291,158.22\n",
            "Iteration 48, Loss: 286,699.06\n",
            "Iteration 49, Loss: 282,713.78\n",
            "Iteration 50, Loss: 278,479.62\n",
            "Iteration 51, Loss: 274,522.59\n",
            "Iteration 52, Loss: 270,890.72\n",
            "Iteration 53, Loss: 267,255.19\n",
            "Iteration 54, Loss: 263,938.59\n",
            "Iteration 55, Loss: 260,629.16\n",
            "Iteration 56, Loss: 257,503.62\n",
            "Iteration 57, Loss: 254,408.45\n",
            "Iteration 58, Loss: 251,482.05\n",
            "Iteration 59, Loss: 248,723.91\n",
            "Iteration 60, Loss: 245,981.42\n",
            "Iteration 61, Loss: 243,419.81\n",
            "Iteration 62, Loss: 240,919.59\n",
            "Iteration 63, Loss: 238,491.78\n",
            "Iteration 64, Loss: 236,195.16\n",
            "Iteration 65, Loss: 233,983.78\n",
            "Iteration 66, Loss: 231,987.08\n",
            "Iteration 67, Loss: 230,278.59\n",
            "Iteration 68, Loss: 229,241.23\n",
            "Iteration 69, Loss: 229,719.19\n",
            "Iteration 70, Loss: 234,134.69\n",
            "Iteration 71, Loss: 245,734.41\n",
            "Iteration 72, Loss: 273,727.50\n",
            "Iteration 73, Loss: 292,306.00\n",
            "Iteration 74, Loss: 294,531.31\n",
            "Iteration 75, Loss: 231,726.33\n",
            "Iteration 76, Loss: 220,369.61\n",
            "Iteration 77, Loss: 255,751.62\n",
            "Iteration 78, Loss: 234,460.62\n",
            "Iteration 79, Loss: 210,742.30\n",
            "Iteration 80, Loss: 228,693.23\n",
            "Iteration 81, Loss: 225,221.44\n",
            "Iteration 82, Loss: 207,403.91\n",
            "Iteration 83, Loss: 215,221.12\n",
            "Iteration 84, Loss: 216,939.44\n",
            "Iteration 85, Loss: 204,427.70\n",
            "Iteration 86, Loss: 206,700.72\n",
            "Iteration 87, Loss: 210,333.64\n",
            "Iteration 88, Loss: 201,835.62\n",
            "Iteration 89, Loss: 200,185.52\n",
            "Iteration 90, Loss: 204,434.22\n",
            "Iteration 91, Loss: 199,840.03\n",
            "Iteration 92, Loss: 195,262.11\n",
            "Iteration 93, Loss: 198,318.34\n",
            "Iteration 94, Loss: 197,779.12\n",
            "Iteration 95, Loss: 192,409.27\n",
            "Iteration 96, Loss: 192,125.16\n",
            "Iteration 97, Loss: 193,897.19\n",
            "Iteration 98, Loss: 191,149.19\n",
            "Iteration 99, Loss: 187,958.39\n",
            "Iteration 100, Loss: 188,044.33\n",
            "Iteration 101, Loss: 188,472.94\n",
            "Iteration 102, Loss: 186,750.44\n",
            "Iteration 103, Loss: 184,351.98\n",
            "Iteration 104, Loss: 183,575.14\n",
            "Iteration 105, Loss: 183,651.08\n",
            "Iteration 106, Loss: 182,910.03\n",
            "Iteration 107, Loss: 181,432.59\n",
            "Iteration 108, Loss: 180,171.45\n",
            "Iteration 109, Loss: 179,437.88\n",
            "Iteration 110, Loss: 178,664.58\n",
            "Iteration 111, Loss: 177,628.78\n",
            "Iteration 112, Loss: 176,689.06\n",
            "Iteration 113, Loss: 176,186.30\n",
            "Iteration 114, Loss: 176,062.31\n",
            "Iteration 115, Loss: 175,953.56\n",
            "Iteration 116, Loss: 175,828.25\n",
            "Iteration 117, Loss: 175,551.86\n",
            "Iteration 118, Loss: 175,929.28\n",
            "Iteration 119, Loss: 177,282.27\n",
            "Iteration 120, Loss: 181,500.53\n",
            "Iteration 121, Loss: 187,859.89\n",
            "Iteration 122, Loss: 200,001.11\n",
            "Iteration 123, Loss: 205,457.02\n",
            "Iteration 124, Loss: 207,581.91\n",
            "Iteration 125, Loss: 186,215.23\n",
            "Iteration 126, Loss: 169,581.70\n",
            "Iteration 127, Loss: 172,613.73\n",
            "Iteration 128, Loss: 183,353.91\n",
            "Iteration 129, Loss: 182,330.56\n",
            "Iteration 130, Loss: 168,754.19\n",
            "Iteration 131, Loss: 166,494.58\n",
            "Iteration 132, Loss: 174,752.72\n",
            "Iteration 133, Loss: 173,891.53\n",
            "Iteration 134, Loss: 165,606.81\n",
            "Iteration 135, Loss: 163,784.80\n",
            "Iteration 136, Loss: 168,828.27\n",
            "Iteration 137, Loss: 168,971.06\n",
            "Iteration 138, Loss: 163,196.73\n",
            "Iteration 139, Loss: 162,205.03\n",
            "Iteration 140, Loss: 165,843.39\n",
            "Iteration 141, Loss: 166,204.78\n",
            "Iteration 142, Loss: 163,022.81\n",
            "Iteration 143, Loss: 163,119.48\n",
            "Iteration 144, Loss: 168,484.94\n",
            "Iteration 145, Loss: 176,842.52\n",
            "Iteration 146, Loss: 181,562.19\n",
            "Iteration 147, Loss: 189,464.59\n",
            "Iteration 148, Loss: 186,856.19\n",
            "Iteration 149, Loss: 179,294.92\n",
            "Iteration 150, Loss: 163,216.61\n",
            "Iteration 151, Loss: 160,882.72\n",
            "Iteration 152, Loss: 171,174.89\n",
            "Iteration 153, Loss: 169,162.61\n",
            "Iteration 154, Loss: 158,514.17\n",
            "Iteration 155, Loss: 156,219.08\n",
            "Iteration 156, Loss: 163,107.05\n",
            "Iteration 157, Loss: 162,575.81\n",
            "Iteration 158, Loss: 154,775.39\n",
            "Iteration 159, Loss: 156,588.94\n",
            "Iteration 160, Loss: 161,636.03\n",
            "Iteration 161, Loss: 157,834.61\n",
            "Iteration 162, Loss: 154,159.78\n",
            "Iteration 163, Loss: 158,603.33\n",
            "Iteration 164, Loss: 161,407.59\n",
            "Iteration 165, Loss: 161,485.89\n",
            "Iteration 166, Loss: 164,239.42\n",
            "Iteration 167, Loss: 173,714.84\n",
            "Iteration 168, Loss: 175,438.47\n",
            "Iteration 169, Loss: 170,268.47\n",
            "Iteration 170, Loss: 160,195.89\n",
            "Iteration 171, Loss: 157,313.19\n",
            "Iteration 172, Loss: 154,207.42\n",
            "Iteration 173, Loss: 151,531.67\n",
            "Iteration 174, Loss: 154,246.94\n",
            "Iteration 175, Loss: 158,261.84\n",
            "Iteration 176, Loss: 156,829.38\n",
            "Iteration 177, Loss: 149,333.67\n",
            "Iteration 178, Loss: 148,169.45\n",
            "Iteration 179, Loss: 152,028.17\n",
            "Iteration 180, Loss: 151,233.62\n",
            "Iteration 181, Loss: 147,261.77\n",
            "Iteration 182, Loss: 146,568.58\n",
            "Iteration 183, Loss: 148,592.59\n",
            "Iteration 184, Loss: 147,728.78\n",
            "Iteration 185, Loss: 145,237.67\n",
            "Iteration 186, Loss: 146,146.73\n",
            "Iteration 187, Loss: 149,997.39\n",
            "Iteration 188, Loss: 152,190.94\n",
            "Iteration 189, Loss: 155,683.80\n",
            "Iteration 190, Loss: 163,432.22\n",
            "Iteration 191, Loss: 184,043.69\n",
            "Iteration 192, Loss: 201,525.19\n",
            "Iteration 193, Loss: 227,523.12\n",
            "Iteration 194, Loss: 205,871.81\n",
            "Iteration 195, Loss: 190,344.03\n",
            "Iteration 196, Loss: 185,048.28\n",
            "Iteration 197, Loss: 184,217.22\n",
            "Iteration 198, Loss: 183,313.59\n",
            "Iteration 199, Loss: 171,601.48\n",
            "Iteration 200, Loss: 170,324.22\n",
            "Iteration 201, Loss: 165,135.39\n",
            "Iteration 202, Loss: 164,800.25\n",
            "Iteration 203, Loss: 164,823.11\n",
            "Iteration 204, Loss: 149,738.23\n",
            "Iteration 205, Loss: 155,223.84\n",
            "Iteration 206, Loss: 163,625.33\n",
            "Iteration 207, Loss: 145,221.84\n",
            "Iteration 208, Loss: 146,733.80\n",
            "Iteration 209, Loss: 157,746.17\n",
            "Iteration 210, Loss: 144,379.81\n",
            "Iteration 211, Loss: 144,009.97\n",
            "Iteration 212, Loss: 149,150.31\n",
            "Iteration 213, Loss: 142,136.06\n",
            "Iteration 214, Loss: 144,836.19\n",
            "Iteration 215, Loss: 144,490.31\n",
            "Iteration 216, Loss: 138,698.97\n",
            "Iteration 217, Loss: 142,761.66\n",
            "Iteration 218, Loss: 142,518.78\n",
            "Iteration 219, Loss: 138,318.48\n",
            "Iteration 220, Loss: 140,342.94\n",
            "Iteration 221, Loss: 139,396.17\n",
            "Iteration 222, Loss: 137,031.59\n",
            "Iteration 223, Loss: 139,501.38\n",
            "Iteration 224, Loss: 140,794.95\n",
            "Iteration 225, Loss: 141,343.78\n",
            "Iteration 226, Loss: 148,263.83\n",
            "Iteration 227, Loss: 157,396.31\n",
            "Iteration 228, Loss: 173,579.64\n",
            "Iteration 229, Loss: 186,039.62\n",
            "Iteration 230, Loss: 195,481.50\n",
            "Iteration 231, Loss: 160,713.38\n",
            "Iteration 232, Loss: 135,832.16\n",
            "Iteration 233, Loss: 150,756.27\n",
            "Iteration 234, Loss: 162,516.47\n",
            "Iteration 235, Loss: 153,689.81\n",
            "Iteration 236, Loss: 150,479.56\n",
            "Iteration 237, Loss: 168,828.84\n",
            "Iteration 238, Loss: 175,377.34\n",
            "Iteration 239, Loss: 167,439.02\n",
            "Iteration 240, Loss: 164,363.23\n",
            "Iteration 241, Loss: 159,400.36\n",
            "Iteration 242, Loss: 141,038.72\n",
            "Iteration 243, Loss: 144,527.28\n",
            "Iteration 244, Loss: 151,897.70\n",
            "Iteration 245, Loss: 145,468.47\n",
            "Iteration 246, Loss: 146,143.91\n",
            "Iteration 247, Loss: 144,061.92\n",
            "Iteration 248, Loss: 136,515.42\n",
            "Iteration 249, Loss: 136,925.31\n",
            "Iteration 250, Loss: 140,871.33\n",
            "Iteration 251, Loss: 138,282.72\n",
            "Iteration 252, Loss: 134,532.41\n",
            "Iteration 253, Loss: 136,886.34\n",
            "Iteration 254, Loss: 135,551.36\n",
            "Iteration 255, Loss: 130,739.45\n",
            "Iteration 256, Loss: 132,509.94\n",
            "Iteration 257, Loss: 133,600.62\n",
            "Iteration 258, Loss: 130,213.62\n",
            "Iteration 259, Loss: 131,492.55\n",
            "Iteration 260, Loss: 133,039.16\n",
            "Iteration 261, Loss: 130,881.40\n",
            "Iteration 262, Loss: 132,596.42\n",
            "Iteration 263, Loss: 135,536.84\n",
            "Iteration 264, Loss: 138,180.86\n",
            "Iteration 265, Loss: 142,584.28\n",
            "Iteration 266, Loss: 150,966.72\n",
            "Iteration 267, Loss: 153,124.62\n",
            "Iteration 268, Loss: 153,303.59\n",
            "Iteration 269, Loss: 139,658.61\n",
            "Iteration 270, Loss: 128,422.47\n",
            "Iteration 271, Loss: 130,299.79\n",
            "Iteration 272, Loss: 135,775.75\n",
            "Iteration 273, Loss: 136,394.66\n",
            "Iteration 274, Loss: 132,884.84\n",
            "Iteration 275, Loss: 130,003.80\n",
            "Iteration 276, Loss: 128,913.14\n",
            "Iteration 277, Loss: 130,477.59\n",
            "Iteration 278, Loss: 130,502.19\n",
            "Iteration 279, Loss: 128,729.76\n",
            "Iteration 280, Loss: 128,204.94\n",
            "Iteration 281, Loss: 126,671.89\n",
            "Iteration 282, Loss: 125,902.42\n",
            "Iteration 283, Loss: 127,613.64\n",
            "Iteration 284, Loss: 127,732.00\n",
            "Iteration 285, Loss: 125,634.95\n",
            "Iteration 286, Loss: 124,245.30\n",
            "Iteration 287, Loss: 124,146.80\n",
            "Iteration 288, Loss: 124,940.39\n",
            "Iteration 289, Loss: 126,065.09\n",
            "Iteration 290, Loss: 126,611.06\n",
            "Iteration 291, Loss: 127,566.39\n",
            "Iteration 292, Loss: 133,444.08\n",
            "Iteration 293, Loss: 145,590.86\n",
            "Iteration 294, Loss: 172,397.27\n",
            "Iteration 295, Loss: 200,576.14\n",
            "Iteration 296, Loss: 235,888.03\n",
            "Iteration 297, Loss: 185,452.47\n",
            "Iteration 298, Loss: 137,329.47\n",
            "Iteration 299, Loss: 153,635.52\n",
            "Iteration 300, Loss: 170,731.80\n",
            "Saved: /content/drive/MyDrive/University of London/Year 3/FINAL PROJECT/Final Project Code/IOB-NST/stylized_outputs/stylized_modern-residential-building-beautiful-recreation-area-modern-residential-building-recreation-area-131403554.jpg_with_Van-Gogh-Starry-Night-Mural-Wallpaper_a08bca97-2248-4dbe-8ea5-5d413f142875.jpg\n"
          ]
        }
      ],
      "source": [
        "# ===================================\n",
        "# MAIN LOOP: APPLY TO ALL IMAGE PAIRS\n",
        "# ===================================\n",
        "\n",
        "# # Sort and match files from content and style directories\n",
        "# content_files = sorted([f for f in os.listdir(content_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
        "# style_files = sorted([f for f in os.listdir(style_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
        "\n",
        "# # Apply NST to each content-style image pair\n",
        "# for content_file, style_file in zip(content_files, style_files):\n",
        "#     content_path = os.path.join(content_dir, content_file)\n",
        "#     style_path = os.path.join(style_dir, style_file)\n",
        "#     output_path = os.path.join(output_dir, f\"stylized_{content_file}_{style_file}\")\n",
        "\n",
        "#     print(f\"\\nProcessing: {content_file} + {style_file}\")\n",
        "#     run_style_transfer(content_path, style_path, output_path, 1e2, 1e5, 1e-6, 300)\n",
        "\n",
        "# Sort all valid image files\n",
        "content_files = sorted([f for f in os.listdir(content_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
        "style_files = sorted([f for f in os.listdir(style_dir) if f.lower().endswith(('.jpg', '.png'))])\n",
        "\n",
        "# Apply each style to each content image\n",
        "for content_file in content_files:\n",
        "    for style_file in style_files:\n",
        "        content_path = os.path.join(content_dir, content_file)\n",
        "        style_path = os.path.join(style_dir, style_file)\n",
        "\n",
        "        # Create a clear output filename\n",
        "        output_name = f\"stylized_{content_file}_with_{style_file}\"\n",
        "        output_path = os.path.join(output_dir, output_name)\n",
        "\n",
        "        print(f\"\\nProcessing: {content_file} + {style_file}\")\n",
        "        run_style_transfer(content_path, style_path, output_path, 1e2, 1e5, 1e-6, 300)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#image is not styled enough - fix!!\n",
        "#display content, style, and result\n",
        "#error graphs\n",
        "#surveys\n",
        "\n",
        "#think about common final dataset for portfolio (check jing review and notes)"
      ],
      "metadata": {
        "id": "NxB1pOv5fyKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}